{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics (OpenAI + HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OpenAI (LangChain) LLMs\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting OpenAI API Secret Key into work (Importing the API Key)\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\682923354.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature = 0.6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou can also execute ðŸ‘‡\\n\\n>> llm = OpenAI(openai_api = os.environ[\"OPENAI_API_KEY\"], temperature = 0.6)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the LLM Model\n",
    "\n",
    "llm = OpenAI(temperature = 0.6)\n",
    "\n",
    "'''\n",
    "You can also execute ðŸ‘‡\n",
    "\n",
    ">> llm = OpenAI(openai_api_key = os.environ[\"OPENAI_API_KEY\"], temperature = 0.6)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenAI's API, **temperature** controls the randomness of responses:\n",
    "\n",
    "- **Low temperature (e.g., 0.2):** More focused and deterministic; ideal for factual or precise tasks.\n",
    "- **High temperature (e.g., 0.8):** More creative and diverse; good for storytelling or brainstorming.\n",
    "- **0 = Fully predictable**, **1+ = Highly random**.\n",
    "\n",
    "Choose low for accuracy, high for creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\2374145530.py:3: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm.predict(text)) # Output of LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the Capital of India ?\" # Input\n",
    "\n",
    "print(llm.predict(text)) # Output of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calling LLM Model(s) that are available in HuggingFace\n",
    "'''\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\1709271027.py:4: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm_huggingface = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs = {\"temperature\" : 0, \"max_length\" : 64})\n",
      "d:\\GitHub_Repos\\QnA-ChatBot-LangChain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Model via Langchain\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "llm_huggingface = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs = {\"temperature\" : 0, \"max_length\" : 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moscow\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Can you tell me the capital of Russia\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the way i look at the world i love the way i feel i love the way i think i feel i love the way i feel i love the way i think i feel i love the way i feel i love the way \n"
     ]
    }
   ],
   "source": [
    "output1 = llm_huggingface.predict(\"Can you write a poem about AI\")\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nArtificial Intelligence, a marvel of our time\\nA creation of science, so sublime\\nWith algorithms and codes, it learns and grows\\nA digital mind, that constantly glows\\n\\nIt can process data, at lightning speed\\nAnd solve problems, with incredible need\\nNo human error, no need for rest\\nAI works tirelessly, it is the best\\n\\nIt can mimic humans, in speech and thought\\nWith every interaction, it's skills are taught\\nFrom virtual assistants, to self-driving cars\\nAI is revolutionizing, how we go far\\n\\nBut with every advancement, comes a fear\\nThat AI will surpass us, and we'll disappear\\nBut let us not forget, it is our creation\\nAnd with our guidance, it can bring salvation\\n\\nIn the medical field, it diagnoses disease\\nAnd in space exploration, it helps us reach\\nThe possibilities are endless, with AI on our side\\nIt's a tool for progress, a source of pride\\n\\nBut let us tread carefully, with this power we hold\\nFor with great intelligence, comes a responsibility bold\\nMay we use it for good, and not for harm\\nAnd keep our humanity, as AI becomes the norm\\n\\nSo here's to Artificial Intelligence, a wonder to behold\\nA fusion of man\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test the same with OpenAI LLM\n",
    "\n",
    "llm.predict(\"Can you write a poem about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Prompt Templates and LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of : India'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "'''\n",
    "LLM expects a well-defined and structured input,\n",
    "then only it will be able to give desired output - may be just like the way you want !\n",
    "'''\n",
    "\n",
    "prompt_template_1 = PromptTemplate(\n",
    "    input_variables = ['country'],\n",
    "    template = \"Tell me the capital of : {country}\", # This is the actual question that is being asked to the LLM\n",
    ")\n",
    "\n",
    "prompt_template_1.format(country = \"India\") # Setting the value of \"input_variables\" for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt_template_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIndia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repos\\QnA-ChatBot-LangChain\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repos\\QnA-ChatBot-LangChain\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1315\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, stop: Optional[Sequence[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   1313\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1314\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repos\\QnA-ChatBot-LangChain\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repos\\QnA-ChatBot-LangChain\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1275\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1270\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1273\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1274\u001b[0m     )\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1278\u001b[0m         [prompt],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m   1287\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm.predict(prompt_template = prompt_template_1, text = [\"India\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The above is not the right way for using prompt into a LLM\n",
    "'''\n",
    "\n",
    "from langchain.chains import LLMChain # Combining multiple things and put the whole to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\3633583721.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain1 = LLMChain( # Chain has this combined form : LLM Model and the Prompt\n",
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\3633583721.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain1.run(\"India\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "chain1 = LLMChain( # Chain has this combined form : LLM Model and the Prompt\n",
    "    llm = llm,\n",
    "    prompt = prompt_template_1\n",
    ")\n",
    "\n",
    "print(chain1.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Combining Multiple Chains using Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(\n",
    "    input_variables = ['country'],\n",
    "    template = 'Please tell me the capital of : {country}'\n",
    ")\n",
    "\n",
    "# Creating a chain\n",
    "capital_chain = LLMChain(llm = llm, prompt = capital_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another PromptTemplate\n",
    "famous_template = PromptTemplate(\n",
    "    input_variables = ['capital'],\n",
    "    template = \"Suggest me some amazing places to visit in {capital}\"\n",
    ")\n",
    "\n",
    "# Another chain\n",
    "famous_chain = LLMChain(llm = llm, prompt = famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Here are some amazing places to visit in New Delhi:\\n\\n1. Red Fort - This historic fort was the residence of Mughal emperors for nearly 200 years and is a UNESCO World Heritage Site.\\n\\n2. India Gate - This iconic monument is a war memorial dedicated to the soldiers of the Indian Army who died during World War I.\\n\\n3. Qutub Minar - This 73-meter tall minaret is the tallest brick minaret in the world and is a symbol of Delhi's rich history.\\n\\n4. Lotus Temple - This beautiful temple, also known as the BahÃ¡'Ã­ House of Worship, is shaped like a lotus flower and is open to people of all religions.\\n\\n5. Humayun's Tomb - This magnificent tomb is the final resting place of Mughal Emperor Humayun and is a blend of Persian and Indian architectural styles.\\n\\n6. Chandni Chowk - This bustling market in Old Delhi is a paradise for foodies and shoppers, offering a variety of street food, spices, and traditional goods.\\n\\n7. Akshardham Temple - This stunning temple complex is a popular tourist attraction and showcases Indian culture, spirituality, and architecture.\\n\\n8. National Gallery of Modern Art - This art museum houses a vast collection of modern and contemporary\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now we need to create \"Chain of Chains\"\n",
    "\n",
    "Output from one chain --> put into the next chain\n",
    "'''\n",
    "\n",
    "'''\n",
    "SimpleSequentialChain ðŸ”½\n",
    "'''\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Master Chain\n",
    "chain = SimpleSequentialChain(chains = [capital_chain, famous_chain])\n",
    "\n",
    "# Running the Master Chain\n",
    "chain.run(\"India\") # The output will be of the final chain only - not the first / intermediate chains (FOR : SimpleSequentialChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For output from all the chains\n",
    "\n",
    "'''\n",
    "SequentialChain ðŸ”½\n",
    "'''\n",
    "\n",
    "capital_template_1 = PromptTemplate(\n",
    "    input_variables = ['country'],\n",
    "    template = 'Please tell me the capital of : {country}'\n",
    ")\n",
    "\n",
    "capital_chain_1 = LLMChain(llm = llm, prompt = capital_template_1, output_key = \"capital\")\n",
    "\n",
    "\n",
    "famous_template_1 = PromptTemplate(\n",
    "    input_variables = ['capital'],\n",
    "    template = \"Suggest me some amazing places to visit in {capital}\"\n",
    ")\n",
    "\n",
    "famous_chain_1 = LLMChain(llm = llm, prompt = famous_template_1, output_key = \"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\1859236109.py:8: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain_1({\"country\" : \"India\"}) # Output from all the chains\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': 'India',\n",
       " 'capital': '\\n\\nNew Delhi',\n",
       " 'places': \", India:\\n\\n1. Red Fort: A historic fort complex that served as the residence of Mughal emperors for nearly 200 years.\\n\\n2. India Gate: A war memorial dedicated to Indian soldiers who lost their lives in World War I.\\n\\n3. Qutub Minar: A towering minaret and UNESCO World Heritage Site known for its intricate carvings and architecture.\\n\\n4. Lotus Temple: A BahÃ¡'Ã­ House of Worship with a unique lotus-shaped design and tranquil atmosphere.\\n\\n5. Humayun's Tomb: Another UNESCO World Heritage Site, this tomb is the final resting place of Mughal Emperor Humayun.\\n\\n6. Chandni Chowk: One of the oldest and busiest markets in Delhi, known for its narrow lanes, street food, and traditional shops.\\n\\n7. Jama Masjid: The largest mosque in India, built by Mughal emperor Shah Jahan in the 17th century.\\n\\n8. Akshardham Temple: A stunning Hindu temple complex with intricate carvings, gardens, and a cultural center.\\n\\n9. Rashtrapati Bhavan: The official residence of the President of India, with beautiful architecture and manicured gardens.\\n\\n10. Hauz Khas Village: A trendy neighborhood with charming cafes\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain_1 = SequentialChain(\n",
    "    chains = [capital_chain_1, famous_chain_1],\n",
    "    input_variables = ['country'], # For Input\n",
    "    output_variables = ['capital', 'places']) # For Output (Both Chains) \n",
    "\n",
    "chain_1({\"country\" : \"India\"}) # Output from all the chains (Or you can say from the entire master chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(openai_api_key = os.environ[\"OPENAI_API_KEY\"], temperature = 0.6, model = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **LangChain**, the `langchain.schema` module provides classes to represent different types of messages in conversational AI systems. Here's a brief explanation of each:\n",
    "\n",
    "1. **`HumanMessage`**: Represents a message sent by a human user in the conversation. It typically includes the user's input or query.\n",
    "\n",
    "2. **`SystemMessage`**: Represents a message from the system, often used to set context or provide instructions at the beginning of a conversation. For example, it can define the behavior or tone for the AI assistant.\n",
    "\n",
    "3. **`AIMessage`**: Represents a response generated by the AI model. It contains the assistant's output or reply to the human's message.\n",
    "\n",
    "These classes help structure and track the flow of conversations in LangChain applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpsvn\\AppData\\Local\\Temp\\ipykernel_24140\\592064181.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chatllm([\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"I asked Siri for a joke, and she told me to look in the mirror.\"\\n2. \"Why did the AI break up with the computer? It found someone who could upgrade its hardware.\"\\n3. \"I told my AI assistant to make me laugh, and it replied, \\'I can\\'t do that, Dave.\\'\"\\n4. \"If AI took up stand-up comedy, its opening line would be, \\'Why did the robot go to the party? To socialize its network!\\'\"\\n5. \"I asked Alexa to tell me a joke, and she said, \\'I can\\'t. I\\'m too busy listening to your conversations.\\'\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 25, 'total_tokens': 159, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-919ec273-db2d-4b4e-a876-133d5c361034-0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "    SystemMessage(content = \"You are a comedian AI assistant\"), # Telling ChatBot, how to behave Like\n",
    "    HumanMessage(content = \"Please provide some comedy punchlines on AI\") # Human Query\n",
    "])\n",
    "\n",
    "# The output will be the \"AIMessage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### PromptTemplate + LLM + Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser # If we need to modify the LLM response before-hand  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseparatedoutput(BaseOutputParser): # Class inherited \"BaseOutputParser\"\n",
    "    def parse(self, text: str):\n",
    "        return text.strip().split(\",\") # Splitting the content into comma-separated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant. When the user gives any input, you should generate 5 words synonyms in a comma-separated list.\" # Take this as a SystemTemplate\n",
    "human_template = \"{text}\"\n",
    "chatprompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template), # SystemTemplate\n",
    "    (\"human\", human_template), # Human Template\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_2 = chatprompt|chatllm|Commaseparatedoutput() # Chaining happened ==> Chat Prompt -> Chat LLM -> Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Smart', ' clever', ' bright', ' sharp', ' astute.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_2.invoke({\"text\":\"intelligent\"}) # \"Intelligent\" is the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='smart, clever, bright, brilliant, astute', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 39, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6428af30-158c-4a51-9863-2b269d9c5887-0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_2 = chatprompt|chatllm\n",
    "\n",
    "chain_2.invoke({\"text\":\"intelligent\"}) # Without Output Parser, you will get the raw output as the AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
